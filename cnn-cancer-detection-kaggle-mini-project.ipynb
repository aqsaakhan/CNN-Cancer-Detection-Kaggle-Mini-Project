{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Histopathologic Cancer Detection - Aqsa Anwar\n\n## 1. Problem and Data Description\n\n### 1.1 Problem Statement\n\nThis project addresses a binary image classification task in the field of digital pathology. The goal is to develop an algorithm that can accurately identify metastatic cancer in small image patches taken from larger digital pathology scans of lymph node sections.\n\nSpecifically, we need to classify whether a given image patch contains metastatic tissue (positive class) or not (negative class). This task is crucial in automating and improving the efficiency of cancer diagnosis, potentially leading to earlier and more accurate detection of metastases.\n\n### 1.2 Dataset Structure\n\nThe dataset provided for this competition is a modified version of the PatchCamelyon (PCam) benchmark dataset. It consists of:\n\n1. Training set: A large number of labeled image patches for model training and validation.\n2. Test set: Unlabeled image patches for final model evaluation.\n\nKey characteristics of the dataset:\n\n- Image format: Small pathology images (presumably in common formats like PNG or JPEG)\n- Image size: Not explicitly stated, but likely to be larger than 32x32 pixels\n- Labels: Binary (0 for non-metastatic, 1 for metastatic)\n- Label file: 'train_labels.csv' provides the ground truth for the training images\n\n### 1.3 Classification Focus\n\nA critical aspect of this classification task is the focus on the center region of each image patch:\n\n- The label (positive or negative) is determined by the presence of metastatic tissue in the center 32x32 pixel region of the patch.\n- Tumor tissue present only in the outer region of the patch does not influence the label.\n- The outer region is provided to enable fully-convolutional models that do not use zero-padding, ensuring consistent behavior when applied to a whole-slide image.\n\nThis focus on the center region adds an interesting dimension to the problem, as models need to learn to concentrate on the most relevant part of the image for accurate classification.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport os\n\n# Set up plotting\nplt.style.use('seaborn')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:28.669262Z","iopub.execute_input":"2024-07-30T18:29:28.670160Z","iopub.status.idle":"2024-07-30T18:29:28.678998Z","shell.execute_reply.started":"2024-07-30T18:29:28.670120Z","shell.execute_reply":"2024-07-30T18:29:28.678052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Analyze Class Distribution","metadata":{}},{"cell_type":"code","source":"# Load the training labels\ntrain_labels = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\n\n# 2.1 Analyze class distribution\nprint(\"\\n2.1 Class Distribution:\")\nclass_distribution = train_labels['label'].value_counts(normalize=True)\nprint(class_distribution)\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x='label', data=train_labels)\nplt.title('Class Distribution in Training Set')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:28.681147Z","iopub.execute_input":"2024-07-30T18:29:28.681538Z","iopub.status.idle":"2024-07-30T18:29:29.192776Z","shell.execute_reply.started":"2024-07-30T18:29:28.681503Z","shell.execute_reply":"2024-07-30T18:29:29.191788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Load and Visualize Images","metadata":{}},{"cell_type":"code","source":"# 2.2 Load and visualize sample images\nprint(\"\\n2.2 Sample Images:\")\n\ndef load_image(image_id):\n    image_path = f'../input/histopathologic-cancer-detection/train/{image_id}.tif'\n    return np.array(Image.open(image_path))\n\n# Sample a few images from each class\npositive_samples = train_labels[train_labels['label'] == 1].sample(4)['id'].values\nnegative_samples = train_labels[train_labels['label'] == 0].sample(4)['id'].values\n\nfig, axes = plt.subplots(2, 4, figsize=(24, 14))  # Increased height\nfig.suptitle(\"Sample Images\", fontsize=24, fontweight='bold')\n\nfor i, image_id in enumerate(positive_samples):\n    axes[0, i].imshow(load_image(image_id))\n    axes[0, i].set_title(f\"Positive Sample {i+1}\", fontsize=16, fontweight='bold')\n    axes[0, i].axis('off')\nfor i, image_id in enumerate(negative_samples):\n    axes[1, i].imshow(load_image(image_id))\n    axes[1, i].set_title(f\"Negative Sample {i+1}\", fontsize=16, fontweight='bold')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.9, hspace=0.3)  # Increased vertical space between subplots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:29.194091Z","iopub.execute_input":"2024-07-30T18:29:29.194496Z","iopub.status.idle":"2024-07-30T18:29:30.227325Z","shell.execute_reply.started":"2024-07-30T18:29:29.194459Z","shell.execute_reply":"2024-07-30T18:29:30.226284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Examine Image Properties","metadata":{}},{"cell_type":"code","source":"# 2.3 Examine image properties\nprint(\"\\n2.3 Image Properties:\")\nsample_image = load_image(train_labels['id'].iloc[0])\nprint(f\"Image shape: {sample_image.shape}\")\nprint(f\"Data type: {sample_image.dtype}\")\nprint(f\"Min pixel value: {sample_image.min()}\")\nprint(f\"Max pixel value: {sample_image.max()}\")\n\n# Display a single image with its center region highlighted\nplt.figure(figsize=(10, 10))\nplt.imshow(sample_image)\ncenter = sample_image.shape[0] // 2\nrect = plt.Rectangle((center-16, center-16), 32, 32, fill=False, edgecolor='red', linewidth=2)\nplt.gca().add_patch(rect)\nplt.title(\"Sample Image with 32x32 Center Region Highlighted\", fontsize=16, fontweight='bold')\nplt.axis('off')\nplt.show()\n\n# Display pixel value distribution\nplt.figure(figsize=(12, 6))\nplt.hist(sample_image.ravel(), bins=256, range=(0, 255))\nplt.title(\"Pixel Value Distribution\", fontsize=16, fontweight='bold')\nplt.xlabel(\"Pixel Value\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:30.229611Z","iopub.execute_input":"2024-07-30T18:29:30.229941Z","iopub.status.idle":"2024-07-30T18:29:31.106732Z","shell.execute_reply.started":"2024-07-30T18:29:30.229914Z","shell.execute_reply":"2024-07-30T18:29:31.105773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Analyze pixel intensity distributions by class","metadata":{}},{"cell_type":"code","source":"# 2.4 Analyze pixel intensity distributions by class\nprint(\"\\n2.4 Pixel Intensity Distribution by Class:\")\n\ndef get_avg_intensity(image_id):\n    img = load_image(image_id)\n    return img.mean()\n\n# Sample a subset of images for faster processing\nsample_size = 1000\nsampled_data = train_labels.sample(sample_size, random_state=42)\n\n# Calculate average intensities\nsampled_data['avg_intensity'] = sampled_data['id'].apply(get_avg_intensity)\n\n# Plot distributions\nplt.figure(figsize=(12, 6))\nsns.histplot(data=sampled_data, x='avg_intensity', hue='label', element='step', stat='density', common_norm=False)\nplt.title('Average Pixel Intensity Distribution by Class', fontsize=16, fontweight='bold')\nplt.xlabel('Average Pixel Intensity')\nplt.ylabel('Density')\nplt.legend(labels=['Negative', 'Positive'])\nplt.show()\n\n# Calculate and print mean intensities for each class\nclass_means = sampled_data.groupby('label')['avg_intensity'].mean()\nprint(\"\\nMean intensities by class:\")\nprint(class_means)\n\n# Perform a t-test to check if the difference is statistically significant\nfrom scipy import stats\nnegative = sampled_data[sampled_data['label'] == 0]['avg_intensity']\npositive = sampled_data[sampled_data['label'] == 1]['avg_intensity']\nt_stat, p_value = stats.ttest_ind(negative, positive)\nprint(f\"\\nt-statistic: {t_stat}\")\nprint(f\"p-value: {p_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:31.108131Z","iopub.execute_input":"2024-07-30T18:29:31.108534Z","iopub.status.idle":"2024-07-30T18:29:33.409339Z","shell.execute_reply.started":"2024-07-30T18:29:31.108491Z","shell.execute_reply":"2024-07-30T18:29:33.408332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 EDA Summary and Implications\n\nBased on our exploratory data analysis, we can draw the following conclusions:\n\n1. **Class Distribution**: \n   The dataset is slightly imbalanced:\n   - Negative class (0): 59.47% of the samples\n   - Positive class (1): 40.53% of the samples\n\n2. **Image Properties**:\n   - Dimensions: 96x96x3 (96x96 pixels with 3 color channels)\n   - Color: The images are in color (RGB)\n   - Pixel Value Range: 0 to 255 (uint8 data type)\n\n3. **Pixel Intensity Analysis**:\n   - Mean intensity for negative class (0): 168.30\n   - Mean intensity for positive class (1): 157.81\n   - The t-test results (t-statistic: 4.27, p-value: 2.13e-05) indicate a statistically significant difference in mean intensities between the two classes.\n   - The distribution plot shows that positive samples tend to have slightly lower average pixel intensities compared to negative samples.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preprocessing and Dataset Preparation\n\nIn this section, we prepare our data for model training by implementing a preprocessing pipeline and setting up our datasets.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Image Preprocessing\n\nWe created a `preprocess_image` function that applies the following steps to each image:\n1. Center crop to 32x32 pixels, focusing on the region of interest\n2. Convert to PyTorch tensor and scale pixel values to the range [0, 1]\n3. Normalize using standard ImageNet mean and standard deviation values\n\nThis preprocessing ensures that all images have consistent dimensions and are properly normalized for our neural network.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\n\ndef preprocess_image(image_id, train_dir='../input/histopathologic-cancer-detection/train/'):\n    # Load image\n    image_path = f'{train_dir}{image_id}.tif'\n    image = Image.open(image_path)\n    \n    # Define preprocessing steps\n    preprocess = transforms.Compose([\n        transforms.CenterCrop(32),  # Crop the center 32x32 region\n        transforms.ToTensor(),      # Convert to tensor and scale to [0, 1]\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n    \n    # Apply preprocessing\n    image_tensor = preprocess(image)\n    \n    return image_tensor\n\n# Test the function\nsample_id = train_labels['id'].iloc[0]\nsample_tensor = preprocess_image(sample_id)\nprint(f\"Preprocessed image shape: {sample_tensor.shape}\")\nprint(f\"Tensor min value: {sample_tensor.min()}\")\nprint(f\"Tensor max value: {sample_tensor.max()}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:33.410679Z","iopub.execute_input":"2024-07-30T18:29:33.410966Z","iopub.status.idle":"2024-07-30T18:29:33.421795Z","shell.execute_reply.started":"2024-07-30T18:29:33.410941Z","shell.execute_reply":"2024-07-30T18:29:33.420841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output shape confirms that we have successfully cropped the image to 32x32 pixels and preserved the 3 color channels. The min and max values indicate that our normalization step has been applied, shifting the pixel values from the original [0, 255] range.\n","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Custom Dataset Class\n\nWe implemented a custom `HistopathDataset` class that:\n- Loads images from file\n- Applies our preprocessing function\n- Pairs images with their corresponding labels\n\nThis class allows for efficient data loading and preprocessing during training.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass HistopathDataset(Dataset):\n    def __init__(self, dataframe, train_dir, transform=None):\n        self.dataframe = dataframe\n        self.train_dir = train_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        img_id = self.dataframe.iloc[idx]['id']\n        label = self.dataframe.iloc[idx]['label']\n        \n        image = preprocess_image(img_id, self.train_dir)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:33.423157Z","iopub.execute_input":"2024-07-30T18:29:33.423628Z","iopub.status.idle":"2024-07-30T18:29:33.434825Z","shell.execute_reply.started":"2024-07-30T18:29:33.423594Z","shell.execute_reply":"2024-07-30T18:29:33.433807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Train-Validation Split\n\nTo evaluate our model's performance, we split our data into training and validation sets:\n- Training set: 80% of the data\n- Validation set: 20% of the data\n\nWe used stratified sampling to ensure that both sets have the same class distribution as the original dataset.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(train_labels, test_size=0.2, random_state=42, stratify=train_labels['label'])\n\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(val_df)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:33.436057Z","iopub.execute_input":"2024-07-30T18:29:33.436453Z","iopub.status.idle":"2024-07-30T18:29:33.539416Z","shell.execute_reply.started":"2024-07-30T18:29:33.436415Z","shell.execute_reply":"2024-07-30T18:29:33.538423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Data Loaders\n\nFinally, we created PyTorch DataLoader objects for both our training and validation sets. These loaders:\n- Batch the data (batch size = 64)\n- Shuffle the training data to prevent overfitting\n- Use multiple workers for efficient data loading\n\nOur preprocessing pipeline ensures that the data is in the correct format for model training, with consistent image sizes, normalized pixel values, and efficient batching.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create datasets\ntrain_dataset = HistopathDataset(train_df, '../input/histopathologic-cancer-detection/train/')\nval_dataset = HistopathDataset(val_df, '../input/histopathologic-cancer-detection/train/')\n\n# Create dataloaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# Check the shape of a batch\nfor images, labels in train_loader:\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:33.540837Z","iopub.execute_input":"2024-07-30T18:29:33.541558Z","iopub.status.idle":"2024-07-30T18:29:33.994781Z","shell.execute_reply.started":"2024-07-30T18:29:33.541518Z","shell.execute_reply":"2024-07-30T18:29:33.993604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This confirms that our DataLoader is correctly batching 64 images at a time, each with dimensions 3x32x32 (channels x height x width), along with their corresponding 64 labels.\n\nOur preprocessing pipeline ensures that the data is in the correct format for model training, with consistent image sizes, normalized pixel values, and efficient batching.","metadata":{}},{"cell_type":"markdown","source":"## 4. Model Architecture and Implementation\n\nFor this histopathologic cancer detection task, we've implemented a Convolutional Neural Network (CNN) using PyTorch. CNNs are well-suited for image classification tasks due to their ability to learn spatial hierarchies of features.\n","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Model Architecture\n\nOur CNN architecture consists of:\n1. Three convolutional layers (32, 64, and 128 filters respectively), each followed by ReLU activation and max pooling.\n2. A fully connected layer with 512 units, followed by ReLU activation and dropout (50% rate) for regularization.\n3. A final fully connected layer with a single output unit.\n4. Sigmoid activation for producing a probability output.\n\nThis architecture progressively increases the number of filters while reducing spatial dimensions, allowing the network to learn increasingly complex features.\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass HistopathCNN(nn.Module):\n    def __init__(self):\n        super(HistopathCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n\n# Instantiate the model\nmodel = HistopathCNN()\nprint(model)\n\n# Calculate the number of trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters: {total_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:33.998582Z","iopub.execute_input":"2024-07-30T18:29:33.998953Z","iopub.status.idle":"2024-07-30T18:29:34.025951Z","shell.execute_reply.started":"2024-07-30T18:29:33.998922Z","shell.execute_reply":"2024-07-30T18:29:34.025024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Model Summary\n- Total trainable parameters: 1,142,849\n\n- The model takes inputs of shape (3, 32, 32), corresponding to our preprocessed image patches, and outputs a single probability value indicating the likelihood of the patch containing metastatic tissue.\n\n### 4.3 Design Considerations\n\n- We used multiple convolutional layers to allow the model to learn hierarchical features.\n- Max pooling helps to reduce spatial dimensions and introduce translation invariance.\n- The number of filters increases from 32 to 64 to 128, allowing the network to learn more complex features as the spatial dimensions decrease.\n- The fully connected layers reduce the features to a single output, with a substantial dropout (p=0.5) to prevent overfitting.\n- The model has over 1.1 million trainable parameters, which should provide sufficient capacity for this task while still being manageable to train.\n\n### 4.4 Potential Improvements\n\n- Given the relatively large number of parameters, we might consider techniques like batch normalization or additional regularization if we observe overfitting during training.\n- Depending on training results, we might adjust the number of filters or layers to find an optimal balance between model capacity and generalization.","metadata":{}},{"cell_type":"markdown","source":"## 5. Model Training and Evaluation\n\nIn this section, we implement the training process for our CNN model.\n\n### 5.1 Training Setup\n\n- **Loss Function**: We use Binary Cross-Entropy Loss, which is suitable for our binary classification task.\n- **Optimizer**: We employ the Adam optimizer with a learning rate of 0.001. Adam is known for its ability to adapt the learning rate for each parameter, often leading to faster convergence.\n- **Device**: We utilize GPU acceleration if available, falling back to CPU if not.\n\n### 5.2 Training and Validation Process\n\nWe implement two main functions:\n1. `train_epoch`: Performs one epoch of training, computing loss and accuracy.\n2. `validate`: Evaluates the model on the validation set without updating weights.\n\nOur training loop runs for 10 epochs, performing the following steps in each epoch:\n1. Train the model on the entire training set.\n2. Validate the model on the validation set.\n3. Print the training and validation loss and accuracy for the epoch.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.nn import BCELoss\nfrom tqdm import tqdm\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move model to device\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training function\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n        inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# Validation function\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Validating\"):\n            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            predicted = (outputs > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# Function to save checkpoint\ndef save_checkpoint(model, optimizer, epoch, val_acc, filename='checkpoint.pth'):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_acc': val_acc,\n    }\n    torch.save(checkpoint, filename)\n    print(f\"Checkpoint saved: {filename}\")\n\n# Function to load checkpoint\ndef load_checkpoint(model, optimizer, filename='checkpoint.pth'):\n    if os.path.isfile(filename):\n        checkpoint = torch.load(filename)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_val_acc = checkpoint['val_acc']\n        print(f\"Checkpoint loaded: {filename}\")\n        return start_epoch, best_val_acc\n    else:\n        return 0, 0\n\n# Training loop\nnum_epochs = 10\nstart_epoch, best_val_acc = load_checkpoint(model, optimizer)\n\nfor epoch in range(start_epoch, num_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    print(\"-\" * 40)\n    \n    # Save checkpoint if validation accuracy improves\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        save_checkpoint(model, optimizer, epoch, val_acc, filename=f'best_model_epoch_{epoch+1}.pth')\n\nprint(\"Training completed!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:29:34.027080Z","iopub.execute_input":"2024-07-30T18:29:34.027372Z","iopub.status.idle":"2024-07-30T19:18:16.484477Z","shell.execute_reply.started":"2024-07-30T18:29:34.027346Z","shell.execute_reply":"2024-07-30T19:18:16.483237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final model performance:\n- Training Accuracy: 88.28%\n- Validation Accuracy: 84.71%\n\n### 5.4 Analysis\n\n1. **Convergence**: The model showed consistent improvement in both training and validation accuracy over the epochs, indicating successful learning.\n\n2. **Overfitting**: There are signs of mild overfitting as the training accuracy (88.28%) is higher than the validation accuracy (84.71%) by the end of training. The gap between training and validation performance started to widen after the 6th epoch.\n\n3. **Performance**: The model achieved a final validation accuracy of 84.71%, which is a significant improvement over the initial validation accuracy of 81.66% in the first epoch.\n\n4. **Learning Dynamics**: \n   - The most rapid improvement occurred in the first 3 epochs.\n   - The best validation performance was achieved in epoch 5 with an accuracy of 85.02%.\n   - After the 5th epoch, the validation performance became somewhat unstable, with fluctuations in accuracy and an increase in loss.\n\n5. **Computation**: Training was performed on a CUDA-enabled GPU. The training speed increased significantly after the first epoch, from about 11.85 iterations/second to around 11-12 iterations/second for subsequent epochs.\n","metadata":{}},{"cell_type":"markdown","source":"## 6. Hyperparameter Tuning\n\nTo improve our model's performance, we implemented a random search for hyperparameter tuning. This process allows us to explore different combinations of hyperparameters and find the set that yields the best performance.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn import BCELoss\nfrom tqdm import tqdm\nimport random\nfrom collections import namedtuple\n\nclass HistopathCNN(nn.Module):\n    def __init__(self, dropout_rate=0.5, fc_units=512):\n        super(HistopathCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, fc_units)\n        self.fc2 = nn.Linear(fc_units, 1)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n\nHyperparameters = namedtuple('Hyperparameters', ['learning_rate', 'dropout_rate', 'fc_units'])\n\ndef create_model(dropout_rate, fc_units):\n    return HistopathCNN(dropout_rate=dropout_rate, fc_units=fc_units)\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n        inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Validating\", leave=False):\n            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            predicted = (outputs > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\ndef train_and_evaluate(model, train_loader, val_loader, learning_rate, num_epochs=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    best_val_acc = 0\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n    \n    return best_val_acc\n\ndef random_search(num_trials=4, num_epochs=3):\n    best_hyperparameters = None\n    best_accuracy = 0\n    \n    for i in range(num_trials):\n        learning_rate = 10 ** random.uniform(-4, -2)  # 1e-4 to 1e-2\n        dropout_rate = random.uniform(0.3, 0.7)\n        fc_units = random.choice([128, 256, 512])\n        \n        hyperparameters = Hyperparameters(learning_rate, dropout_rate, fc_units)\n        model = create_model(dropout_rate, fc_units)\n        accuracy = train_and_evaluate(model, train_loader, val_loader, learning_rate, num_epochs=num_epochs)\n        \n        print(f\"Trial {i+1}/{num_trials}\")\n        print(f\"Hyperparameters: {hyperparameters}\")\n        print(f\"Validation Accuracy: {accuracy:.4f}\")\n        print(\"-\" * 40)\n        \n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_hyperparameters = hyperparameters\n            \n            # Save the best model and hyperparameters\n            torch.save({\n                'hyperparameters': best_hyperparameters,\n                'model_state_dict': model.state_dict(),\n                'accuracy': best_accuracy\n            }, 'best_model_checkpoint.pth')\n    \n    return best_hyperparameters, best_accuracy\n\n# Run the random search\nbest_hyperparameters, best_accuracy = random_search(num_trials=4, num_epochs=3)\n\nprint(\"Best Hyperparameters:\")\nprint(best_hyperparameters)\nprint(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n\n# Load the best model (optional)\ncheckpoint = torch.load('best_model_checkpoint.pth')\nbest_model = create_model(checkpoint['hyperparameters'].dropout_rate, checkpoint['hyperparameters'].fc_units)\nbest_model.load_state_dict(checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:18:16.486281Z","iopub.execute_input":"2024-07-30T19:18:16.486700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1 Hyperparameter Tuning (Partial Results)\n\nWe've completed 2 full trials and part of a third trial:\n\n1. Trial 1:\n   - Learning rate: 0.0002467509155819963\n   - Dropout rate: 0.45099534223734367\n   - FC units: 512\n   - Best Validation Accuracy: 0.8288 (82.88%)\n\n2. Trial 2:\n   - Learning rate: 0.0004248563359482669\n   - Dropout rate: 0.5174733766056243\n   - FC units: 256\n   - Best Validation Accuracy: 0.8384 (83.84%)\n\n3. Trial 3 (incomplete due to computational constraints):\n   - Learning rate and dropout rate not provided\n   - FC units: not provided\n   - Last recorded Validation Accuracy: 0.8280 (82.80%)\n\n### 6.2 Analysis of Partial Results\n\n1. **Performance Improvement**: Both completed trials showed improvement over the epochs, with Trial 2 achieving the best validation accuracy of 83.84%.\n\n2. **Learning Rate**: The learning rate in Trial 2 (0.00042) was higher than in Trial 1 (0.00025), and it resulted in better performance. This suggests that a slightly higher learning rate might be beneficial for this task.\n\n3. **Dropout Rate**: Trial 2 had a higher dropout rate (0.517 vs 0.451) and performed better. This indicates that the model might benefit from stronger regularization.\n\n4. **Network Capacity**: Interestingly, the model with fewer units in the fully connected layer (256 in Trial 2 vs 512 in Trial 1) performed better. This suggests that the larger model might be overfitting, and a more compact architecture could be more suitable.\n\n5. **Training Duration**: Each trial ran for 3 epochs. While this gives us a good initial comparison, longer training might reveal different trends or further improvements.","metadata":{}},{"cell_type":"markdown","source":"## 7. Final Model Training\n\nWe trained our final model using the best hyperparameters for 10 epochs:","metadata":{}},{"cell_type":"code","source":"# Use the best hyperparameters we found\nbest_lr = 0.0004248563359482669\nbest_dropout = 0.5174733766056243\nbest_fc_units = 256\n\n# Create the final model\nfinal_model = HistopathCNN(dropout_rate=best_dropout, fc_units=best_fc_units).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(final_model.parameters(), lr=best_lr)\n\n# Train for 10 epochs\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(final_model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate(final_model, val_loader, criterion, device)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    print(\"-\" * 40)\n\n# Final evaluation\nfinal_val_loss, final_val_acc = validate(final_model, val_loader, criterion, device)\nprint(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n\n# Save the model\ntorch.save(final_model.state_dict(), 'final_histopath_model.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport seaborn as sns\nimport numpy as np\n\n\n# 1. Training and Validation Curves\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 2. ROC Curve and AUC\nval_labels, val_preds = evaluate_model(final_model, val_loader, device)\nfpr, tpr, _ = roc_curve(val_labels, val_preds)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")\n\n# 3. Confusion Matrix\nval_pred_labels = (val_preds > 0.5).astype(int)\ncm = confusion_matrix(val_labels, val_pred_labels)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n# 4. Misclassification Analysis\nmisclassified_indices = np.where(val_labels != val_pred_labels)[0]\nnum_display = min(5, len(misclassified_indices))\n\nplt.figure(figsize=(15, 3))\nfor i, idx in enumerate(misclassified_indices[:num_display]):\n    plt.subplot(1, num_display, i+1)\n    img, label = val_dataset[idx]\n    plt.imshow(img.permute(1, 2, 0))\n    plt.title(f\"True: {label}, Pred: {val_pred_labels[idx]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions for Kaggle submission\ntest_preds = predict_test(final_model, test_loader, device)\nsubmission = pd.DataFrame({'id': test_ids, 'label': test_preds.squeeze()})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}